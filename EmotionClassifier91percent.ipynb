# Step 1: Import necessary libraries
import numpy as np
import pandas as pd
import os
import librosa
import tensorflow as tf
from tqdm import tqdm
from python_speech_features import mfcc
from joblib import Parallel, delayed

# Import Keras model layers and utilities
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense, Input, BatchNormalization, LSTM, TimeDistributed
from keras.models import Sequential
from keras.optimizers import Adam
from keras.regularizers import l2  # For L2 regularization
from keras.utils import to_categorical
from keras.callbacks import EarlyStopping

# Check for GPU availability
print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
    print('This runtime is not configured to use a GPU.')
else:
    print('GPU device is available at:', device_name)

# Step 2: Mount Google Drive to access the dataset (Colab-specific)
from google.colab import drive
drive.mount('/content/drive')

# Step 3: Copy dataset from Google Drive to Colab's local runtime (for faster access)
!cp -r /content/drive/MyDrive/Audio_DatasetMini100 /content/

# Step 4: Load data from the local directory
root_dir = '/content/Audio_DatasetMini100'  # Path to your data directory

# Initialize lists for file paths and labels
paths = []
labels = []

# Load files and their labels
for emotion_folder in os.listdir(root_dir):
    folder_path = os.path.join(root_dir, emotion_folder)
    if os.path.isdir(folder_path):
        for filename in os.listdir(folder_path):
            file_path = os.path.join(folder_path, filename)
            paths.append(file_path)
            label = emotion_folder.replace("_emotion", "").lower()
            labels.append(label)

# Create a DataFrame with paths and labels
df = pd.DataFrame({'path': paths, 'label': labels})
df['Length'] = [librosa.get_duration(path=path) for path in df['path']]
print(df.head())

# Model Configurations
class Config:
    def __init__(self, mode='hybrid', nfilt=26, nfeat=13, nfft=512, rate=16000):
        self.mode = mode
        self.nfilt = nfilt
        self.nfeat = nfeat
        self.nfft = nfft
        self.rate = rate
        self.step = int(rate / 10)

config = Config(mode='hybrid')

# Calculate class distribution for balanced training
class_dist = df.groupby(['label'])['Length'].mean()
prop_dist = class_dist / class_dist.sum()
classes = list(np.unique(df.label))

# Parallelized feature extraction function
def extract_features(file_path, label, config):
    signal, rate = librosa.load(file_path, sr=config.rate)
    rand_index = np.random.randint(0, signal.shape[0] - config.step)
    sample = signal[rand_index: rand_index + config.step]
    X_sample = mfcc(sample, rate, numcep=config.nfeat, nfilt=config.nfilt, nfft=config.nfft).T
    return X_sample, label

# Build dataset using parallel processing
def build_rand_feat_parallel(df, n_samples, config, class_dist, prop_dist, classes, mixup_alpha=0.2):
    results = Parallel(n_jobs=-1)(
        delayed(extract_features)(df.loc[np.random.choice(df[df.label == rand_class].index), 'path'],
                                  classes.index(rand_class), config)
        for rand_class in np.random.choice(class_dist.index, size=n_samples, p=prop_dist)
    )
    X, y = zip(*results)
    X, y = np.array(X), np.array(y)

    # Normalizing the features
    _min, _max = np.min(X), np.max(X)
    X = (X - _min) / (_max - _min)
    X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)
    y = to_categorical(y, num_classes=len(classes))
    return X, y

# Generate random features using the parallelized function
n_samples = 2 * int(df['Length'].sum() / 0.1)
X, y = build_rand_feat_parallel(df, n_samples, config, class_dist, prop_dist, classes)

# Define the CNN model architecture optimized for GPU
def get_conv_model(input_shape):
    model = Sequential()
    model.add(Input(shape=input_shape))
    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.0005)))
    model.add(BatchNormalization())
    model.add(Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.0005)))
    model.add(BatchNormalization())
    model.add(Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.0005)))
    model.add(BatchNormalization())
    model.add(Conv2D(256, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.0005)))
    model.add(BatchNormalization())
    model.add(Conv2D(512, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.0005)))
    model.add(BatchNormalization())
    model.add(Conv2D(1024, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.0005)))
    model.add(BatchNormalization())
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.4))
    model.add(Flatten())
    model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.0005)))
    model.add(Dropout(0.4))
    model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.0005)))
    model.add(Dense(len(classes), activation='softmax'))

    optimizer = Adam(learning_rate=0.0001)
    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    return model

# Build the CNN model
input_shape = (X.shape[1], X.shape[2], 1)
model = get_conv_model(input_shape)

# Early stopping to prevent overfitting
early_stopping = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)

# Train the model using tf.data.Dataset for efficient data loading
dataset = tf.data.Dataset.from_tensor_slices((X, y)).batch(64).prefetch(tf.data.AUTOTUNE)
model.fit(dataset, epochs=40, verbose=1, callbacks=[early_stopping])
