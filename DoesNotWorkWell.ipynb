import numpy as np
import pandas as pd
import os
import librosa
import tensorflow as tf
from tqdm import tqdm
from python_speech_features import mfcc
from joblib import Parallel, delayed


# Import Keras model layers and utilities
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense, Input, BatchNormalization, LSTM, TimeDistributed
from keras.models import Sequential
from keras.optimizers import Adam
from keras.regularizers import l2  # Import for L2 regularization
from keras.utils import to_categorical
from keras.callbacks import EarlyStopping

# Step 1: Verify GPU availability
print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))

# Step 2 (Optional): Mount Google Drive to access the dataset (Colab-specific)
# Assuming you're using Colab, uncomment these lines to mount your drive
# from google.colab import drive
# drive.mount('/content/drive')

# Step 3 (Optional): Copy dataset from Google Drive to the local runtime (for faster access)
# Uncomment these lines if your data is in a folder named Audio_DatasetMini100 on your Drive
# !cp -r /content/drive/MyDrive/Audio_DatasetMini100 /content/

# Step 4: Load data from the local directory
root_dir = '/content/Audio_DatasetMini100'  # Replace with your data directory path

# Step 5: Initialize lists for file paths and labels
paths = []
labels = []

# Loop through each folder (representing an emotion) and each file
for emotion_folder in os.listdir(root_dir):
    folder_path = os.path.join(root_dir, emotion_folder)
    if os.path.isdir(folder_path):
        for filename in os.listdir(folder_path):
            file_path = os.path.join(folder_path, filename)
            paths.append(file_path)
            label = emotion_folder.replace("_emotion", "").lower()
            labels.append(label)

# Create a DataFrame with paths and labels
df = pd.DataFrame({'path': paths, 'label': labels})
df['Length'] = [librosa.get_duration(path=path) for path in df['path']]
print(df.head())

# Model Configurations
class Config:
    def __init__(self, mode='hybrid', nfilt=26, nfeat=13, nfft=512, rate=16000):
        self.mode = mode
        self.nfilt = nfilt
        self.nfeat = nfeat
        self.nfft = nfft
        self.rate = rate
        self.step = int(rate / 10)

config = Config(mode='hybrid')

# Calculate class distribution for balanced training
class_dist = df.groupby(['label'])['Length'].mean()
prop_dist = class_dist / class_dist.sum()
classes = list(np.unique(df.label))

# Parallelized feature extraction function
def extract_features(file_path, label, config):
    signal, rate = librosa.load(file_path, sr=config.rate)
    rand_index = np.random.randint(0, signal.shape[0] - config.step)
    sample = signal[rand_index: rand_index + config.step]
    X_sample = mfcc(sample, rate, numcep=config.nfeat, nfilt=config.nfilt, nfft=config.nfft).T
    return X_sample, label

# Build dataset using parallel processing
def build_rand_feat_parallel(df, n_samples, config, class_dist, prop_dist, classes, mixup_alpha=0.2):
    results = Parallel(n_jobs=-1)(
        delayed(extract_features)(df.loc[np.random.choice(df[df.label == rand_class].index), 'path'],
                                 classes.index(rand_class), config)
        for rand_class in np.random.choice(class_dist.index, size=n_samples, p=prop_dist)
    )
    X, y = zip(*results)
    X, y = np.array(X), np.array(y)

    # Apply data augmentation techniques
    augmented_X = []
    augmented_y = []
    for i in range(len(X)):
        # Randomly select another sample
        j = np.random.randint(0, len(X))

        # Interpolate between the two samples
        lambda_ = np.random.beta(mixup_alpha, mixup_alpha)
        mixed_x = lambda_ * X[i] + (1 - lambda_) * X[j]
        mixed_y = lambda_ * y[i] + (1 - lambda_) * y[j]

        # Add noise (reduced strength)
        mixed_x = mixed_x + np.random.normal(0, 0.005, mixed_x.shape)  # Adjust noise standard deviation

        # Time shifting (reduced range)
        shift = np.random.randint(-config.step // 8, config.step // 8)  # Adjust shift range
        mixed_x = np.roll(mixed_x, shift, axis=0)

        # Pitch shifting (optional)
        # mixed_x = librosa.effects.pitch_shift(mixed_x, sr=config.rate, n_steps=np.random.randint(-2, 2))

        augmented_X.append(mixed_x)
        augmented_y.append(mixed_y)

    X = np.array(augmented_X)
    y = np.array(augmented_y)

    # Normalizing the features
    _min, _max = np.min(X), np.max(X)
    X = (X - _min) / (_max - _min)
    X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)
    y = to_categorical(y, num_classes=len(classes))
    return X, y

# Generate random features using the parallelized function
n_samples = 2 * int(df['Length'].sum() / 0.1)
X, y = build_rand_feat_parallel(df, n_samples, config, class_dist, prop_dist, classes)

# Define the hybrid CNN-LSTM model architecture
def get_hybrid_model(input_shape):
    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape))
    model.add(BatchNormalization())
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization())
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(TimeDistributed(Flatten()))

    model.add(LSTM(128, return_sequences=True))
    model.add(Dropout(0.5))
    model.add(LSTM(64, return_sequences=False))
    model.add(Dropout(0.5))

    model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))
    model.add(Dense(len(classes), activation='softmax'))

    optimizer = Adam(learning_rate=0.0001)
    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    return model

# Build the hybrid CNN-LSTM model
input_shape = (X.shape[1], X.shape[2], 1)
model = get_hybrid_model(input_shape)

# Early stopping to avoid overfitting
early_stopping = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)

# Train the model using tf.data.Dataset for efficient loading
dataset = tf.data.Dataset.from_tensor_slices((X, y)).batch(64).prefetch(tf.data.AUTOTUNE)

model.fit(dataset, epochs=40, verbose=1, callbacks=[early_stopping])

# Hyperparameter tuning (example with RandomizedSearchCV)
from sklearn.model_selection import RandomizedSearchCV
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier

def create_model(learning_rate, dropout_rate, units):
    # ... (define your model architecture here)
    # ... (use the provided learning rate, dropout rate, and units)

    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.0005)))
    model.add(BatchNormalization())
    model.add(Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.0005)))
    model.add(BatchNormalization())
    model.add(Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.0005)))
    model.add(BatchNormalization())
    model.add(Conv2D(256, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.0005)))
    model.add(BatchNormalization())
    model.add(Conv2D(512, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.0005)))
    model.add(BatchNormalization())
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(dropout_rate))
    model.add(Flatten())
    model.add(Dense(units, activation='relu', kernel_regularizer=l2(0.0005)))
    model.add(Dropout(dropout_rate))
    model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.0005)))
    model.add(Dense(len(classes), activation='softmax'))

    optimizer = Adam(learning_rate=learning_rate)
    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    return model

# Create KerasClassifier
model = KerasClassifier(build_fn=create_model, verbose=0)

# Define hyperparameter grid
param_grid = {
    'learning_rate': [0.001, 0.0001, 0.00001],
    'dropout_rate': [0.2, 0.3, 0.4],
    'units': [64, 128, 256]
}

# Create RandomizedSearchCV object
grid_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=10, cv=5)

# Perform hyperparameter tuning
grid_search.fit(X_train, y_train)

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the best model on the validation set
val_loss, val_acc = best_model.evaluate(X_val, y_val)
print("Validation accuracy:", val_acc)

# Make predictions on the test set
test_loss, test_acc = best_model.evaluate(X_test, y_test)
print("Test accuracy:", test_acc)
