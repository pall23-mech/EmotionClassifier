import numpy as np
import pandas as pd
import os
import librosa
import tensorflow as tf
from tqdm import tqdm
from python_speech_features import mfcc
from joblib import Parallel, delayed  # New imports for parallel processing

# MODEL 
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense, Input, BatchNormalization
from keras.models import Sequential
from keras.optimizers import Adam  # New import for learning rate adjustments
from keras.utils import to_categorical

# Set TensorFlow threading configuration to use all 8 CPU cores
tf.config.threading.set_intra_op_parallelism_threads(8)  # Use 8 threads for operations within a single op
tf.config.threading.set_inter_op_parallelism_threads(8)  # Use 8 threads for operations across different ops

# Load data from folders
root_dir = r'C:\Code\Audio_DatasetMini100'  # Use raw string to handle Windows backslashes properly

paths = []
labels = []

# Loop through each folder (representing an emotion) and each file
for emotion_folder in os.listdir(root_dir):
    folder_path = os.path.join(root_dir, emotion_folder)
    
    # Check if it's a directory (i.e., an emotion folder)
    if os.path.isdir(folder_path):
        for filename in os.listdir(folder_path):
            file_path = os.path.join(folder_path, filename)
            paths.append(file_path)
            
            # Use the folder name (without "_emotion") as the label
            label = emotion_folder.replace("_emotion", "").lower()
            labels.append(label)

# Create DataFrame with paths and labels
df = pd.DataFrame()
df['path'] = paths
df['label'] = labels
df['Length'] = [librosa.get_duration(path=path) for path in df['path']]

# Display the first few rows to verify the data
print(df.head())

# Model Configurations
class Config:
    def __init__(self, mode='conv', nfilt=26, nfeat=13, nfft=512, rate=16000):
        self.mode = mode
        self.nfilt = nfilt
        self.nfeat = nfeat
        self.nfft = nfft
        self.rate = rate
        self.step = int(rate / 10)

config = Config(mode='conv')

# Correct the calculation of prop_dist to ensure it sums to 1
class_dist = df.groupby(['label'])['Length'].mean()
prop_dist = class_dist / class_dist.sum()  # Normalize the distribution to sum to 1
classes = list(np.unique(df.label))

# Parallelized feature extraction for efficiency
def extract_features(file_path, label, config):
    signal, rate = librosa.load(file_path, sr=config.rate)
    rand_index = np.random.randint(0, signal.shape[0] - config.step)
    sample = signal[rand_index: rand_index + config.step]
    X_sample = mfcc(sample, rate, numcep=config.nfeat, nfilt=config.nfilt, nfft=config.nfft).T
    return X_sample, label

# Modified build_rand_feat function to use parallel processing
def build_rand_feat_parallel(df, n_samples, config, class_dist, prop_dist, classes):
    results = Parallel(n_jobs=-1)(
        delayed(extract_features)(df.loc[np.random.choice(df[df.label == rand_class].index), 'path'], 
                                  classes.index(rand_class), config)
        for rand_class in np.random.choice(class_dist.index, size=n_samples, p=prop_dist)
    )
    
    X, y = zip(*results)
    X, y = np.array(X), np.array(y)

    # Normalizing the features
    _min, _max = np.min(X), np.max(X)
    X = (X - _min) / (_max - _min)
    
    # Reshape for Conv2D input
    X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)
    y = to_categorical(y, num_classes=len(classes))
    return X, y

# Generate random features using the parallelized function
n_samples = 2 * int(df['Length'].sum() / 0.1)
X, y = build_rand_feat_parallel(df, n_samples, config, class_dist, prop_dist, classes)

# Model Building
input_shape = (X.shape[1], X.shape[2], 1)

def get_conv_model(input_shape):
    model = Sequential()
    model.add(Input(shape=input_shape))
    model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization())  # Added Batch Normalization after Conv2D layer
    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization())  # Added Batch Normalization after Conv2D layer
    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization())  # Added Batch Normalization after Conv2D layer
    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.5))
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(len(classes), activation='softmax'))

    # Use a lower learning rate for better convergence
    optimizer = Adam(learning_rate=0.0001)  # Lower learning rate for stable training
    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    return model

# Build the CNN model
model = get_conv_model(input_shape)

# Train the model
model.fit(X, y, epochs=40, batch_size=64, shuffle=True)
