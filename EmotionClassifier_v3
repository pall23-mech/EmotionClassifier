import numpy as np 
import pandas as pd 
import os
import seaborn as sns 
import matplotlib.pyplot as plt
from tqdm import tqdm 
from scipy.io import wavfile 
from python_speech_features import mfcc, logfbank
import librosa

# MODEL 
from keras.layers import Conv2D, MaxPooling2D, Flatten, LSTM, BatchNormalization
from keras.layers import Dropout, Dense, TimeDistributed
from keras.models import Sequential
from keras.utils import to_categorical
from sklearn.utils.class_weight import compute_class_weight

# Load data from folders
root_dir = r'C:\Code\Audio_Dataset'  # Use raw string to handle Windows backslashes properly

paths = []
labels = []

# Loop through each folder (representing an emotion) and each file
for emotion_folder in os.listdir(root_dir):
    folder_path = os.path.join(root_dir, emotion_folder)
    
    # Check if it's a directory (i.e., an emotion folder)
    if os.path.isdir(folder_path):
        for filename in os.listdir(folder_path):
            file_path = os.path.join(folder_path, filename)
            paths.append(file_path)
            
            # Use the folder name (without "_emotion") as the label
            label = emotion_folder.replace("_emotion", "").lower()
            labels.append(label)

# Create DataFrame with paths and labels
df = pd.DataFrame()
df['path'] = paths
df['label'] = labels
df['Length'] = [librosa.get_duration(path=path) for path in df['path']]

# Display the first few rows to verify the data
print(df.head())

# Visualization and Signal Processing Functions
def plot_signals(signals):
    fig, axes = plt.subplots(nrows=2, ncols=3, sharex=False, sharey=True, figsize=(20,5))
    fig.suptitle('Time Series', fontsize=16)
    i = 0
    for x in range(2):
        for y in range(3):
            if i < len(signals):
                axes[x, y].set_title(list(signals.keys())[i])
                axes[x, y].plot(list(signals.values())[i])
                i += 1
            axes[x, y].get_xaxis().set_visible(False)
            axes[x, y].get_yaxis().set_visible(False)

def plot_fft(fft):
    fig, axes = plt.subplots(nrows=2, ncols=3, sharex=False, sharey=True, figsize=(20,5))
    fig.suptitle('Fourier Transforms', fontsize=16)
    i = 0
    for x in range(2):
        for y in range(3):
            if i < len(fft):
                data = list(fft.values())[i]
                Y, freq = data[0], data[1]
                axes[x, y].set_title(list(fft.keys())[i])
                axes[x, y].plot(freq, Y)
                i += 1
            axes[x, y].get_xaxis().set_visible(False)
            axes[x, y].get_yaxis().set_visible(False)

def plot_fbank(fbank):
    fig, axes = plt.subplots(nrows=2, ncols=3, sharex=False, sharey=True, figsize=(20,5))
    fig.suptitle('Filter Bank Coefficients', fontsize=16)
    i = 0
    for x in range(2):
        for y in range(3):
            if i < len(fbank):
                axes[x, y].set_title(list(fbank.keys())[i])
                axes[x, y].imshow(list(fbank.values())[i], cmap='hot', interpolation='nearest')
                i += 1
            axes[x, y].get_xaxis().set_visible(False)
            axes[x, y].get_yaxis().set_visible(False)

def plot_mfccs(mfccs):
    fig, axes = plt.subplots(nrows=2, ncols=3, sharex=False, sharey=True, figsize=(20,5))
    fig.suptitle('Mel Frequency Cepstrum Coefficients', fontsize=16)
    i = 0
    for x in range(2):
        for y in range(3):
            if i < len(mfccs):
                axes[x, y].set_title(list(mfccs.keys())[i])
                axes[x, y].imshow(list(mfccs.values())[i], cmap='hot', interpolation='nearest')
                i += 1
            axes[x, y].get_xaxis().set_visible(False)
            axes[x, y].get_yaxis().set_visible(False)
            
def calc_fft(y, rate):
    n = len(y)
    freq = np.fft.rfftfreq(n, d=1/rate)
    Y = abs(np.fft.rfft(y)/n)
    return (Y, freq)

def envelope(y, rate, threshold):
    mask = []
    y = pd.Series(y).apply(np.abs)
    y_mean = y.rolling(window=int(rate/10), min_periods=1, center=True).mean()
    for mean in y_mean:
        if mean > threshold:
            mask.append(True)
        else:
            mask.append(False)
    return mask

# Data Distribution Visualization
sns.countplot(x='label', data=df)
plt.title('Distribution of Data')
plt.show()

classes = list(np.unique(df.label))
class_dist = df.groupby(['label'])['Length'].mean()

fig, ax = plt.subplots()  
ax.set_title('Class Distribution', y=1.08)
ax.pie(class_dist, labels=class_dist.index, autopct='%1.1f%%', shadow=False, startangle=90)
ax.axis('equal')
plt.show()

# Audio Signal Processing
signals, fft, fbank, mfccs = {}, {}, {}, {}

for c in classes:
    wav_file = df[df.label == c].iloc[0, 0]  
    signal, rate = librosa.load(wav_file, sr=22500)
    
    mask = envelope(signal, rate, 0.005)
    signal = signal[mask]
    
    signals[c] = signal
    fft[c] = calc_fft(signal, rate)
    fbank[c] = logfbank(signal[:rate], rate, nfilt=26, nfft=1103).T
    mfccs[c] = mfcc(signal[:rate], rate, numcep=13, nfilt=26, nfft=1103).T

plot_signals(signals)
plot_fft(fft)
plot_fbank(fbank)
plot_mfccs(mfccs)

# Model Configurations
class Config:
    def __init__(self, mode='conv', nfilt=26, nfeat=13, nfft=512, rate=16000):
        self.mode = mode
        self.nfilt = nfilt
        self.nfeat = nfeat
        self.nfft = nfft
        self.rate = rate
        self.step = int(rate/10)

config = Config(mode='conv')

# Feature Extraction and Model Training
def build_rand_feat(df, n_samples, config, class_dist, prop_dist, classes):
    X = []
    y = []
    _min, _max = float('inf'), -float('inf')
    for _ in tqdm(range(n_samples)):
        rand_class = np.random.choice(class_dist.index, p=prop_dist)
        file = np.random.choice(df[df.label == rand_class].index)
        wav_file = df.loc[file, 'path']
        signal, rate = librosa.load(wav_file, sr=config.rate)
        label = df.at[file, 'label']
        rand_index = np.random.randint(0, signal.shape[0] - config.step)
        sample = signal[rand_index: rand_index + config.step]
        X_sample = mfcc(sample, rate, numcep=config.nfeat, nfilt=config.nfilt, nfft=config.nfft).T
        _min = min(np.amin(X_sample), _min)
        _max = max(np.amax(X_sample), _max)
        X.append(X_sample if config.mode == 'conv' else X_sample.T)
        y.append(classes.index(label))
    X, y = np.array(X), np.array(y)
    X = (X - _min) / (_max - _min)
    if config.mode == 'conv':
        X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)
    elif config.mode == 'time':
        X = X.reshape(X.shape[0], X.shape[1], X.shape[2])
    y = to_categorical(y, num_classes=len(classes))
    return X, y

# Generate random features
n_samples = 2 * int(df['Length'].sum() / 0.1)
prop_dist = class_dist / class_dist.sum()
X, y = build_rand_feat(df, n_samples, config, class_dist, prop_dist, classes)

# Model Building
input_shape = (X.shape[1], X.shape[2], 1)

def get_conv_model(input_shape):
    model = Sequential()
    model.add(Conv2D(32, (3,3), activation ='relu', strides=(1,1), padding='same', input_shape=input_shape))
    model.add(Conv2D(64, (3,3), activation = 'relu', strides=(1,1), padding='same'))
    model.add(Conv2D(128, (3,3), activation = 'relu', strides=(1,1), padding='same'))
    model.add(Conv2D(256, (3,3), activation = 'relu', strides=(1,1), padding='same'))
    model.add(MaxPooling2D((2,2)))
    model.add(Dropout(0.5))
    model.add(Flatten())
    model.add(Dense(128, activation = 'relu'))
    model.add(Dense(64, activation = 'relu'))
    model.add(Dense(6, activation = 'softmax'))
    model.compile(loss = 'categorical_crossentropy', optimizer= 'adam', metrics=['accuracy'])
    return model 

# Build the CNN model
model = get_conv_model(input_shape)

# Train the model
model.fit(X, y, epochs=40, batch_size=64, shuffle=True)
